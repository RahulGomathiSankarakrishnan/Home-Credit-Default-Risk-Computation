{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w58LQkTVmJd4"
      },
      "source": [
        "# Home Credit Default Risk (HCDR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGWETjW5mJd7"
      },
      "source": [
        "The course project is based on the [Home Credit Default Risk (HCDR)  Kaggle Competition](https://www.kaggle.com/c/home-credit-default-risk/). The goal of this project is to predict whether or not a client will repay a loan. In order to make sure that people who struggle to get loans due to insufficient or non-existent credit histories have a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n",
        "\n",
        "\n",
        "## Some of the challenges\n",
        "\n",
        "1. Dataset size \n",
        "   * (688 meg uncompressed) with millions of rows of data\n",
        "   * 2.71 Gig of data uncompressed\n",
        "* Dealing with missing data\n",
        "* Imbalanced datasets\n",
        "* Summarizing transaction data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN8gUCsDmJd8"
      },
      "source": [
        "# Kaggle API setup\n",
        "Kaggle is a Data Science Competition Platform which shares a lot of datasets. In the past, it was troublesome to submit your result as your have to go through the console in your browser and drag your files there. Now you can interact with Kaggle via the command line. E.g., \n",
        "\n",
        "```bash\n",
        "! kaggle competitions files home-credit-default-risk\n",
        "```\n",
        "\n",
        "It is quite easy to setup, it takes me less than 15 minutes to finish a submission.\n",
        "\n",
        "1. Install library\n",
        "* Create a API Token (edit your profile on [Kaggle.com](https://www.kaggle.com/)); this produces `kaggle.json` file\n",
        "* Put your JSON `kaggle.json` in the right place\n",
        "* Access competition files; make submissions via the command (see examples below)\n",
        "* Submit result\n",
        "\n",
        "For more detailed information on setting the Kaggle API see [here](https://medium.com/@nokkk/make-your-kaggle-submissions-with-kaggle-official-api-f49093c04f8a) and [here](https://github.com/Kaggle/kaggle-api).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0_WJBS5mJd8"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_GTd00TmJd9"
      },
      "outputs": [],
      "source": [
        "!pwd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtQHsMglmJd9"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /root/shared/Downloads/kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAHIx9hgmJd-"
      },
      "outputs": [],
      "source": [
        "! kaggle competitions files home-credit-default-risk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaXnWVzWmJd-"
      },
      "source": [
        "# Dataset and how to download\n",
        "\n",
        "\n",
        "## Back ground Home Credit Group\n",
        "\n",
        "Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n",
        "\n",
        "### Home Credit Group\n",
        "\n",
        "Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n",
        "\n",
        "While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n",
        "\n",
        "\n",
        "## Background on the dataset\n",
        "Home Credit is a non-banking financial institution, founded in 1997 in the Czech Republic.\n",
        "\n",
        "The company operates in 14 countries (including United States, Russia, Kazahstan, Belarus, China, India) and focuses on lending primarily to people with little or no credit history which will either not obtain loans or became victims of untrustworthly lenders.\n",
        "\n",
        "Home Credit group has over 29 million customers, total assests of 21 billions Euro, over 160 millions loans, with the majority in Asia and and almost half of them in China (as of 19-05-2018).\n",
        "\n",
        "While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n",
        "\n",
        "## Data files overview\n",
        "There are 7 different sources of data:\n",
        "\n",
        "* __application_train/application_test:__ the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training application data comes with the TARGET indicating __0: the loan was repaid__ or __1: the loan was not repaid__. The target variable defines if the client had payment difficulties meaning he/she had late payment more than X days on at least one of the first Y installments of the loan. Such case is marked as 1 while other all other cases as 0.\n",
        "* __bureau:__ data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n",
        "* __bureau_balance:__ monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length.\n",
        "* __previous_application:__ previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.\n",
        "* __POS_CASH_BALANCE:__ monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n",
        "* credit_card_balance: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n",
        "* __installments_payment:__ payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9QxJBYsmJd_"
      },
      "outputs": [],
      "source": [
        "# ![alt](home_credit.png \"Home credit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I29VtZ7mJeA"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysxAoSZpmJeA"
      },
      "source": [
        "## Downloading the files via Kaggle API\n",
        "\n",
        "Create a base directory:\n",
        "\n",
        "```bash\n",
        "DATA_DIR = \"../../../Data/home-credit-default-risk\"   #same level as course repo in the data directory\n",
        "```\n",
        "\n",
        "Please download the project data files and data dictionary and unzip them using either of the following approaches:\n",
        "\n",
        "1. Click on the `Download` button on the following [Data Webpage](https://www.kaggle.com/c/home-credit-default-risk/data) and unzip the  zip file to the `BASE_DIR`\n",
        "2. If you plan to use the Kaggle API, please use the following steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wAvfiz3mJeA"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"../../../Data/home-credit-default-risk\"   #same level as course repo in the data directory\n",
        "#DATA_DIR = os.path.join('./ddddd/')\n",
        "!mkdir $DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoCFVsIzmJeB"
      },
      "outputs": [],
      "source": [
        "!ls -l $DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv-hQwVmmJeB"
      },
      "outputs": [],
      "source": [
        "! kaggle competitions download home-credit-default-risk -p $DATA_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-iwTAA-mJeB"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLkHe48omJeB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "import zipfile\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJqGi_PRmJeC"
      },
      "outputs": [],
      "source": [
        "unzippingReq = False\n",
        "if unzippingReq: #please modify this code \n",
        "    zip_ref = zipfile.ZipFile('application_train.csv.zip', 'r')\n",
        "    zip_ref.extractall('datasets')\n",
        "    zip_ref.close()\n",
        "    zip_ref = zipfile.ZipFile('application_test.csv.zip', 'r')\n",
        "    zip_ref.extractall('datasets')\n",
        "    zip_ref.close()\n",
        "    zip_ref = zipfile.ZipFile('bureau_balance.csv.zip', 'r')\n",
        "    zip_ref.extractall('datasets')\n",
        "    zip_ref.close()\n",
        "    zip_ref = zipfile.ZipFile('bureau.csv.zip', 'r')\n",
        "    zip_ref.extractall('datasets')\n",
        "    zip_ref.close()\n",
        "    zip_ref = zipfile.ZipFile('credit_card_balance.csv.zip', 'r')\n",
        "    zip_ref.extractall('datasets')\n",
        "    zip_ref.close()\n",
        "    zip_ref = zipfile.ZipFile('installments_payments.csv.zip', 'r')\n",
        "    zip_ref.extractall('datasets')\n",
        "    zip_ref.close()\n",
        "    zip_ref = zipfile.ZipFile('POS_CASH_balance.csv.zip', 'r')\n",
        "    zip_ref.extractall('datasets')\n",
        "    zip_ref.close()\n",
        "    zip_ref = zipfile.ZipFile('previous_application.csv.zip', 'r')\n",
        "    zip_ref.extractall('datasets')\n",
        "    zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQv7rVMFmJeC"
      },
      "source": [
        "## Data files overview\n",
        "### Data Dictionary\n",
        "\n",
        "As part of the data download comes a  Data Dictionary. It named `HomeCredit_columns_description.csv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2WHlRTWmJeC"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPWdMI2hmJeC"
      },
      "source": [
        "### Application train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6jOFqiOmJeC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "import zipfile\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def load_data(in_path, name):\n",
        "    df = pd.read_csv(in_path)\n",
        "    print(f\"{name}: shape is {df.shape}\")\n",
        "    print(df.info())\n",
        "    display(df.head(5))\n",
        "    return df\n",
        "\n",
        "datasets={}  # lets store the datasets in a dictionary so we can keep track of them easily\n",
        "ds_name = 'application_train'\n",
        "datasets[ds_name] = load_data(os.path.join(DATA_DIR, f'{ds_name}.csv'), ds_name)\n",
        "\n",
        "datasets['application_train'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_KJvqOomJeC"
      },
      "source": [
        "### Application test\n",
        "\n",
        "\n",
        "* __application_train/application_test:__ the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training application data comes with the TARGET indicating __0: the loan was repaid__ or __1: the loan was not repaid__. The target variable defines if the client had payment difficulties meaning he/she had late payment more than X days on at least one of the first Y installments of the loan. Such case is marked as 1 while other all other cases as 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1jq5ZIAmJeC"
      },
      "outputs": [],
      "source": [
        "ds_name = 'application_test'\n",
        "datasets[ds_name] = load_data(os.path.join(DATA_DIR, f'{ds_name}.csv'), ds_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zxZ1DlNmJeD"
      },
      "source": [
        "The application dataset has the most information about the client: Gender, income, family status, education ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va5st0HlmJeD"
      },
      "source": [
        "### The Other datasets\n",
        "\n",
        "* __bureau:__ data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n",
        "* __bureau_balance:__ monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length.\n",
        "* __previous_application:__ previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.\n",
        "* __POS_CASH_BALANCE:__ monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n",
        "* credit_card_balance: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n",
        "* __installments_payment:__ payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb-auRwrmJeD"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "ds_names = (\"application_train\", \"application_test\", \"bureau\",\"bureau_balance\",\"credit_card_balance\",\"installments_payments\",\n",
        "            \"previous_application\",\"POS_CASH_balance\")\n",
        "\n",
        "for ds_name in ds_names:\n",
        "    datasets[ds_name] = load_data(os.path.join(DATA_DIR, f'{ds_name}.csv'), ds_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coEXTE_amJeD"
      },
      "outputs": [],
      "source": [
        "for ds_name in datasets.keys():\n",
        "    print(f'dataset {ds_name:24}: [ {datasets[ds_name].shape[0]:10,}, {datasets[ds_name].shape[1]}]')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-jgqORymJeD"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nik7TgkrmJeD"
      },
      "source": [
        "## Summary of Application train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7Y9ctEhmJeD"
      },
      "outputs": [],
      "source": [
        "datasets[\"application_train\"].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7WBIB_1mJeD"
      },
      "outputs": [],
      "source": [
        "datasets[\"application_train\"].describe() #numerical only features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYhgnuOdmJeD"
      },
      "outputs": [],
      "source": [
        "datasets[\"application_test\"].describe() #numerical only features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stiUoEKPmJeD"
      },
      "outputs": [],
      "source": [
        "#datasets[\"application_train\"].describe(include='all') #look at all categorical and numerical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPH21CjYmJeD"
      },
      "source": [
        "## Missing data for application train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzW6JeY9mJeD"
      },
      "outputs": [],
      "source": [
        "percent = (datasets[\"application_train\"].isnull().sum()/datasets[\"application_train\"].isnull().count()*100).sort_values(ascending = False).round(2)\n",
        "sum_missing = datasets[\"application_train\"].isna().sum().sort_values(ascending = False)\n",
        "missing_application_train_data  = pd.concat([percent, sum_missing], axis=1, keys=['Percent', \"Train Missing Count\"])\n",
        "missing_application_train_data.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZojw1SmJeE"
      },
      "outputs": [],
      "source": [
        "percent = (datasets[\"application_test\"].isnull().sum()/datasets[\"application_test\"].isnull().count()*100).sort_values(ascending = False).round(2)\n",
        "sum_missing = datasets[\"application_test\"].isna().sum().sort_values(ascending = False)\n",
        "missing_application_train_data  = pd.concat([percent, sum_missing], axis=1, keys=['Percent', \"Test Missing Count\"])\n",
        "missing_application_train_data.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ortK0EpLmJeE"
      },
      "source": [
        "## Distribution of the target column "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-s_5ibSmJeE"
      },
      "outputs": [],
      "source": [
        "datasets[\"application_train\"]['TARGET'].astype(int).plot.hist();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xva_vUUwmJeE"
      },
      "source": [
        "## Correlation with  the target column "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6Clss8umJeE"
      },
      "outputs": [],
      "source": [
        "correlations = datasets[\"application_train\"].corr()['TARGET'].sort_values()\n",
        "print('Most Positive Correlations:\\n', correlations.tail(10))\n",
        "print('\\nMost Negative Correlations:\\n', correlations.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTsTkDVHmJeE"
      },
      "source": [
        "## Applicants Age "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGHU6gHKmJeE"
      },
      "outputs": [],
      "source": [
        "plt.hist(datasets[\"application_train\"]['DAYS_BIRTH'] / -365, edgecolor = 'k', bins = 25)\n",
        "plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSggRItBmJeE"
      },
      "source": [
        "## Applicants occupations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7xnvu5VmJeE"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x='OCCUPATION_TYPE', data=datasets[\"application_train\"]);\n",
        "plt.title('Applicants Occupation');\n",
        "plt.xticks(rotation=90);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ialEX2KzmJeE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzf2XfoumJeE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BIlMWg7mJeE"
      },
      "source": [
        "# Dataset questions\n",
        "## Unique record for each SK_ID_CURR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnLht4bzmJeE"
      },
      "outputs": [],
      "source": [
        "datasets.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1teXBeOmJeF"
      },
      "outputs": [],
      "source": [
        "len(datasets[\"application_train\"][\"SK_ID_CURR\"].unique()) == datasets[\"application_train\"].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGOxGU4ImJeF"
      },
      "outputs": [],
      "source": [
        "np.intersect1d(datasets[\"application_train\"][\"SK_ID_CURR\"], datasets[\"application_test\"][\"SK_ID_CURR\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZt_tHz_mJeF"
      },
      "outputs": [],
      "source": [
        "datasets[\"application_test\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaodV42amJeF"
      },
      "outputs": [],
      "source": [
        "datasets[\"application_train\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp3Hj25nmJeF"
      },
      "source": [
        "## previous applications for the submission file\n",
        "The persons in the kaggle submission file have had previous applications in the `previous_application.csv`. 47,800 out 48,744 people have had previous appications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cGMrEPbmJeF"
      },
      "outputs": [],
      "source": [
        "appsDF.shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qfMnGvFmJeF"
      },
      "outputs": [],
      "source": [
        "appsDF = datasets[\"previous_application\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPHtqfvWmJeF"
      },
      "outputs": [],
      "source": [
        "len(np.intersect1d(datasets[\"previous_application\"][\"SK_ID_CURR\"], datasets[\"application_test\"][\"SK_ID_CURR\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_euTinjBmJeF"
      },
      "outputs": [],
      "source": [
        "print(f\"There are  {appsDF.shape[0]:,} previous applications\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPxsdisymJeF"
      },
      "outputs": [],
      "source": [
        "# How many entries are there for each month?\n",
        "prevAppCounts = appsDF['SK_ID_CURR'].value_counts(dropna=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP8G-BUPmJeF"
      },
      "outputs": [],
      "source": [
        "len(prevAppCounts[prevAppCounts >40])  #more that 40 previous applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceYNIjGimJeF"
      },
      "outputs": [],
      "source": [
        "prevAppCounts[prevAppCounts >50].plot(kind='bar')\n",
        "plt.xticks(rotation=25)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnJFTckEmJeF"
      },
      "source": [
        "### Histogram of Number of previous applications for an ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUIwqgDpmJeF"
      },
      "outputs": [],
      "source": [
        "sum(appsDF['SK_ID_CURR'].value_counts()==1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VGMGZv-imJeF"
      },
      "outputs": [],
      "source": [
        "plt.hist(appsDF['SK_ID_CURR'].value_counts(), cumulative =True, bins = 100);\n",
        "plt.grid()\n",
        "plt.ylabel('cumulative number of IDs')\n",
        "plt.xlabel('Number of previous applications per ID')\n",
        "plt.title('Histogram of Number of previous applications for an ID')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_H6fcrrmJeF"
      },
      "source": [
        "##### **Can we differentiate applications by low, medium and high previous apps?**\n",
        "    * Low = <5 claims (22%)\n",
        "    * Medium = 10 to 39 claims (58%)\n",
        "    * High = 40 or more claims (20%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5zwQVXamJeG"
      },
      "outputs": [],
      "source": [
        "apps_all = appsDF['SK_ID_CURR'].nunique()\n",
        "apps_5plus = appsDF['SK_ID_CURR'].value_counts()>=5\n",
        "apps_40plus = appsDF['SK_ID_CURR'].value_counts()>=40\n",
        "print('Percentage with 10 or more previous apps:', np.round(100.*(sum(apps_5plus)/apps_all),5))\n",
        "print('Percentage with 40 or more previous apps:', np.round(100.*(sum(apps_40plus)/apps_all),5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsSktTfKmJeG"
      },
      "source": [
        "# Joining secondary tables with the primary table\n",
        "\n",
        "In the case of the HCDR competition (and many other machine learning problems that involve multiple tables in 3NF or not)  we need to join these datasets (denormalize) when using a machine learning pipeline. Joining the secondary tables with the primary table will lead to lots of new features about each loan application; these features will tend to be aggregate type features or meta data about the loan or its application. How can we do this when using Machine Learning Pipelines?\n",
        "\n",
        "## Joining `previous_application` with `application_x`\n",
        "We refer to the `application_train` data (and also `application_test` data also) as the **primary table** and the other files as the **secondary tables** (e.g., `previous_application` dataset). All tables can be joined using the primary key `SK_ID_PREV`.\n",
        "\n",
        "Let's assume we wish to generate a feature based on previous application attempts. In this case, possible features here could be:\n",
        "\n",
        "* A simple feature could be the number of previous applications.\n",
        "* Other summary features of original features such as `AMT_APPLICATION`, `AMT_CREDIT` could be based on average, min, max, median, etc.\n",
        " \n",
        "To build such features, we need to join the `application_train` data (and also `application_test` data also) with the 'previous_application' dataset (and the other available datasets).\n",
        "\n",
        "When joining this data in the context of pipelines, different strategies come to mind with various tradeoffs:\n",
        "\n",
        "1. Preprocess each of the non-application data sets, thereby generating many new (derived) features, and then joining (aka merge) the results with the `application_train` data (the labeled dataset) and with the `application_test` data (the unlabeled submission dataset) prior to processing the data (in a train, valid, test partition) via your machine learning pipeline. [This approach is recommended for this HCDR competition. WHY?]\n",
        "\n",
        "* Do the joins as part of the transformation steps. [Not recommended here. WHY?]. How can this be done? Will it work?\n",
        "  * This would be necessary if we had dataset wide features such as IDF (inverse document frequency) which depend on the entire subset of data as opposed to a single loan application (e.g., a feature about the relative amount applied for such as the percentile of the loan amount being applied for).\n",
        " \n",
        "I want you to think about this section and build on this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esCFCEY7mJeG"
      },
      "source": [
        "## Roadmap for secondary table processing\n",
        "\n",
        "1. Transform all the secondary tables to features that can be joined into the main table the application table (labeled and unlabeled)\n",
        "   * 'bureau', 'bureau_balance', 'credit_card_balance', 'installments_payments', \n",
        "   * 'previous_application', 'POS_CASH_balance'\n",
        "* Merge the transformed secondary tables with the primary tables (i.e., the `application_train` data (the labeled dataset) and with the `application_test` data (the unlabeled submission dataset)), thereby leading to X_train, y_train, X_valid, etc.\n",
        "* Proceed with the learning pipeline using X_train, y_train, X_valid, etc.\n",
        "* Generate a submission file using the learnt model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd-bctWzmJeG"
      },
      "source": [
        "## agg detour\n",
        "\n",
        "Aggregate using one or more operations over the specified axis.\n",
        "\n",
        "For more details see [agg](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html)\n",
        "```python\n",
        "DataFrame.agg(func, axis=0, *args, **kwargs**)\n",
        "```\n",
        "Aggregate using one or more operations over the specified axis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZEr2R6fmJeG"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame([[1, 2, 3],\n",
        "                    [4, 5, 6],\n",
        "                   [7, 8, 9],\n",
        "                   [np.nan, np.nan, np.nan]],\n",
        "                   columns=['A', 'B', 'C'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP1n0_A1mJeG"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCDII5lsmJeG"
      },
      "outputs": [],
      "source": [
        "df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n",
        "#        A    B\n",
        "#max   NaN  8.0\n",
        "#min   1.0  2.0\n",
        "#sum  12.0  NaN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUveRpAZmJeG"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'A': [1, 1, 2, 2],\n",
        "                    'B': [1, 2, 3, 4],\n",
        "                    'C': np.random.randn(4)})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAPWHBNSmJeG"
      },
      "outputs": [],
      "source": [
        "df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})\n",
        "#    B             C\n",
        "#  min max       sum\n",
        "#A\n",
        "#1   1   2  0.590716\n",
        "#2   3   4  0.704907"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3p8IOlymJeG"
      },
      "outputs": [],
      "source": [
        "appsDF.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT7-adrumJeG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b224d7jmJeG"
      },
      "outputs": [],
      "source": [
        "funcs = [\"a\",\"b\",\"c\"]\n",
        "{f:f\"{f}_max\" for f in funcs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZYVvJHHmJeG"
      },
      "source": [
        "### Multiple condition expressions in Pandas\n",
        "So far, both our boolean selections have involved a single condition. You can, of course, have as many conditions as you would like. To do so, you will need to combine your boolean expressions using the three logical operators and, or and not.\n",
        "\n",
        "Use &, | , ~\n",
        "Although Python uses the syntax and, or, and not, these will not work when testing multiple conditions with pandas. The details of why are explained [here](https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-39e811c81a0c).\n",
        "\n",
        "You must use the following operators with pandas:\n",
        "\n",
        "* & for and\n",
        "* | for or\n",
        "* ~ for not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHa52bNAmJeG"
      },
      "outputs": [],
      "source": [
        "appsDF[0:50][(appsDF[\"SK_ID_CURR\"]==175704)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO_BMPWbmJeG"
      },
      "outputs": [],
      "source": [
        "appsDF[0:50][(appsDF[\"SK_ID_CURR\"]==175704) & ~(appsDF[\"AMT_CREDIT\"]==1.0)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDyap9aImJeG"
      },
      "source": [
        "## Missing values in prevApps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya3ToJf0mJeG"
      },
      "outputs": [],
      "source": [
        "appsDF.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOIaU4D2mJeH"
      },
      "outputs": [],
      "source": [
        "appsDF.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bvhc7gdmJeH"
      },
      "source": [
        "## feature engineering for prevApp table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viweCv0QmJeH"
      },
      "outputs": [],
      "source": [
        "agg_op_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcZALG1wmJeH"
      },
      "outputs": [],
      "source": [
        "features = ['AMT_ANNUITY', 'AMT_APPLICATION']\n",
        "agg_op_features = {}\n",
        "for f in features: #build agg dictionary\n",
        "    agg_op_features[f] = {f\"{f}_{func}\":func for func in [\"min\", \"max\", \"mean\"]}\n",
        "print(f\"{appsDF[features].describe()}\")\n",
        "result = appsDF.groupby([\"SK_ID_CURR\"]).agg(agg_op_features)\n",
        "result.columns = result.columns.droplevel() #drop 1 of the header row but keep the feature name header row\n",
        "result = result.reset_index(level=[\"SK_ID_CURR\"])\n",
        "result['range_AMT_APPLICATION'] = result['AMT_APPLICATION_max'] - result['AMT_APPLICATION_min']\n",
        "print(f\"result.shape: {result.shape}\")\n",
        "result[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-Vj_-BXmJeH"
      },
      "outputs": [],
      "source": [
        "result.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImOdDMtpmJeH"
      },
      "source": [
        "## feature transformer for prevApp table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-FDx2NomJeH"
      },
      "outputs": [],
      "source": [
        "# Create aggregate features (via pipeline)\n",
        "class prevAppsFeaturesAggregater(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, features=None): # no *args or **kargs\n",
        "        self.features = features\n",
        "        self.agg_op_features = {}\n",
        "        for f in features:\n",
        "            self.agg_op_features[f] = {f\"{f}_{func}\":func for func in [\"min\", \"max\", \"mean\"]}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit         \n",
        "        result = X.groupby([\"SK_ID_CURR\"]).agg(self.agg_op_features)\n",
        "        result.columns = result.columns.droplevel()\n",
        "        result = result.reset_index(level=[\"SK_ID_CURR\"])\n",
        "        result['range_AMT_APPLICATION'] = result['AMT_APPLICATION_max'] - result['AMT_APPLICATION_min']\n",
        "        return result # return dataframe with the join key \"SK_ID_CURR\"\n",
        "    \n",
        "\n",
        "from sklearn.pipeline import make_pipeline \n",
        "def test_driver_prevAppsFeaturesAggregater(df, features):\n",
        "    print(f\"df.shape: {df.shape}\\n\")\n",
        "    print(f\"df[{features}][0:5]: \\n{df[features][0:5]}\")\n",
        "    test_pipeline = make_pipeline(prevAppsFeaturesAggregater(features))\n",
        "    return(test_pipeline.fit_transform(df))\n",
        "         \n",
        "features = ['AMT_ANNUITY', 'AMT_APPLICATION']\n",
        "features = ['AMT_ANNUITY',\n",
        "       'AMT_APPLICATION', 'AMT_CREDIT', 'AMT_DOWN_PAYMENT', 'AMT_GOODS_PRICE',\n",
        "       'RATE_DOWN_PAYMENT', 'RATE_INTEREST_PRIMARY',\n",
        "       'RATE_INTEREST_PRIVILEGED', 'DAYS_DECISION', 'NAME_PAYMENT_TYPE',\n",
        "       'CNT_PAYMENT', \n",
        "       'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION',\n",
        "       'DAYS_LAST_DUE', 'DAYS_TERMINATION']\n",
        "features = ['AMT_ANNUITY', 'AMT_APPLICATION']\n",
        "res = test_driver_prevAppsFeaturesAggregater(appsDF, features)\n",
        "print(f\"HELLO\")\n",
        "print(f\"Test driver: \\n{res[0:10]}\")\n",
        "print(f\"input[features][0:10]: \\n{appsDF[0:10]}\")\n",
        "\n",
        "\n",
        "# QUESTION, should we lower case df['OCCUPATION_TYPE'] as Sales staff != 'Sales Staff'? (hint: YES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX-mHXbGmJeH"
      },
      "source": [
        "## Join the labeled dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9skKbXh6mJeH"
      },
      "outputs": [],
      "source": [
        "~3==3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjryIkZMmJeH"
      },
      "outputs": [],
      "source": [
        "datasets.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0pX29odmJeH"
      },
      "outputs": [],
      "source": [
        "\n",
        "features = ['AMT_ANNUITY', 'AMT_APPLICATION']\n",
        "prevApps_feature_pipeline = Pipeline([\n",
        "        ('prevApps_add_features1', prevApps_add_features1()),  # add some new features \n",
        "        ('prevApps_add_features2', prevApps_add_features2()),  # add some new features\n",
        "        ('prevApps_aggregater', prevAppsFeaturesAggregater()), # Aggregate across old and new features\n",
        "    ])\n",
        "\n",
        "\n",
        "X_train= datasets[\"application_train\"] #primary dataset\n",
        "appsDF = datasets[\"previous_application\"] #prev app\n",
        "\n",
        "\n",
        "merge_all_data = False\n",
        "\n",
        "# transform all the secondary tables\n",
        "# 'bureau', 'bureau_balance', 'credit_card_balance', 'installments_payments', \n",
        "# 'previous_application', 'POS_CASH_balance'\n",
        "\n",
        "if merge_all_data:\n",
        "    prevApps_aggregated = prevApps_feature_pipeline.transform(appsDF)\n",
        "    \n",
        "    #'bureau', 'bureau_balance', 'credit_card_balance', 'installments_payments', \n",
        "    # 'previous_application', 'POS_CASH_balance'\n",
        "\n",
        "# merge primary table and secondary tables using features based on meta data and  aggregage stats \n",
        "if merge_all_data:\n",
        "    # 1. Join/Merge in prevApps Data\n",
        "    X_train = X_train.merge(prevApps_aggregated, how='left', on='SK_ID_CURR')\n",
        "\n",
        "    # 2. Join/Merge in ...... Data\n",
        "    #X_train = X_train.merge(...._aggregated, how='left', on=\"SK_ID_CURR\")\n",
        "\n",
        "    # 3. Join/Merge in .....Data\n",
        "    #dX_train = X_train.merge(...._aggregated, how='left', on=\"SK_ID_CURR\")\n",
        "\n",
        "    # 4. Join/Merge in Aggregated ...... Data\n",
        "    #X_train = X_train.merge(...._aggregated, how='left', on=\"SK_ID_CURR\")\n",
        "\n",
        "    # ......"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnSZb3tvmJeH"
      },
      "source": [
        "## Join the unlabeled dataset (i.e., the submission file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TbGRzfMmJeH"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_kaggle_test= datasets[\"application_test\"]\n",
        "if merge_all_data:\n",
        "    # 1. Join/Merge in prevApps Data\n",
        "    X_kaggle_test = X_kaggle_test.merge(prevApps_aggregated, how='left', on='SK_ID_CURR')\n",
        "\n",
        "    # 2. Join/Merge in ...... Data\n",
        "    #X_train = X_train.merge(...._aggregated, how='left', on=\"SK_ID_CURR\")\n",
        "\n",
        "    # 3. Join/Merge in .....Data\n",
        "    #df_labeled = df_labeled.merge(...._aggregated, how='left', on=\"SK_ID_CURR\")\n",
        "\n",
        "    # 4. Join/Merge in Aggregated ...... Data\n",
        "    #df_labeled = df_labeled.merge(...._aggregated, how='left', on=\"SK_ID_CURR\")\n",
        "\n",
        "    # ......"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JusiRZGtmJeH"
      },
      "outputs": [],
      "source": [
        "# approval rate 'NFLAG_INSURED_ON_APPROVAL'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xWxPiPjmJeH"
      },
      "outputs": [],
      "source": [
        "# Convert categorical features to numerical approximations (via pipeline)\n",
        "class ClaimAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None): \n",
        "        charlson_idx_dt = {'0': 0, '1-2': 2, '3-4': 4, '5+': 6}\n",
        "        los_dt = {'1 day': 1, '2 days': 2, '3 days': 3, '4 days': 4, '5 days': 5, '6 days': 6,\n",
        "          '1- 2 weeks': 11, '2- 4 weeks': 21, '4- 8 weeks': 42, '26+ weeks': 180}\n",
        "        X['PayDelay'] = X['PayDelay'].apply(lambda x: int(x) if x != '162+' else int(162))\n",
        "        X['DSFS'] = X['DSFS'].apply(lambda x: None if pd.isnull(x) else int(x[0]) + 1)\n",
        "        X['CharlsonIndex'] = X['CharlsonIndex'].apply(lambda x: charlson_idx_dt[x])\n",
        "        X['LengthOfStay'] = X['LengthOfStay'].apply(lambda x: None if pd.isnull(x) else los_dt[x])\n",
        "        return X\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJuSFJPZmJeH"
      },
      "source": [
        "# Processing pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1h0yjKsmJeI"
      },
      "source": [
        "###  OHE when previously unseen unique values in the test/validation set\n",
        "\n",
        "Train, validation and Test sets (and the leakage problem we have mentioned previously):\n",
        "\n",
        " \n",
        "\n",
        "Let's look at a small usecase to tell us how to deal with this:\n",
        "\n",
        "* The OneHotEncoder is fitted to the training set, which means that for each unique value present in the training set, for each feature, a new column is created. Let's say we have 39 columns after the encoding up from 30 (before preprocessing).\n",
        "* The output is a numpy array (when the option sparse=False is used), which has the disadvantage of losing all the information about the original column names and values.\n",
        "* When we try to transform the test set, after having fitted the encoder to the training set, we obtain a `ValueError`. This is because the there are new, previously unseen unique values in the test set and the encoder doesnâ€™t know how to handle these values. In order to use both the transformed training and test sets in machine learning algorithms, we need them to have the same number of columns.\n",
        "\n",
        "This last problem can be solved by using the option handle_unknown='ignore'of the OneHotEncoder, which, as the name suggests, will ignore previously unseen values when transforming the test set.\n",
        "\n",
        " \n",
        "\n",
        "Here is a example that in action:\n",
        "\n",
        "```python\n",
        "# Identify the categorical features we wish to consider.\n",
        "cat_attribs = ['CODE_GENDER', 'FLAG_OWN_REALTY','FLAG_OWN_CAR','NAME_CONTRACT_TYPE', \n",
        "               'NAME_EDUCATION_TYPE','OCCUPATION_TYPE','NAME_INCOME_TYPE']\n",
        "\n",
        "# Notice handle_unknown=\"ignore\" in OHE which ignore values from the validation/test that\n",
        "# do NOT occur in the training set\n",
        "cat_pipeline = Pipeline([\n",
        "        ('selector', DataFrameSelector(cat_attribs)),\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzQIUpkImJeI",
        "outputId": "e69ea9f9-128b-4d8a-b720-8a6dabd5788a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bp</th>\n",
              "      <th>sg</th>\n",
              "      <th>al</th>\n",
              "      <th>su</th>\n",
              "      <th>rbc</th>\n",
              "      <th>pc</th>\n",
              "      <th>pcc</th>\n",
              "      <th>ba</th>\n",
              "      <th>bgr</th>\n",
              "      <th>...</th>\n",
              "      <th>pcv</th>\n",
              "      <th>wbcc</th>\n",
              "      <th>rbcc</th>\n",
              "      <th>htn</th>\n",
              "      <th>dm</th>\n",
              "      <th>cad</th>\n",
              "      <th>appet</th>\n",
              "      <th>pe</th>\n",
              "      <th>ane</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>48</td>\n",
              "      <td>80</td>\n",
              "      <td>1.020</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>?</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>121</td>\n",
              "      <td>...</td>\n",
              "      <td>44</td>\n",
              "      <td>7800</td>\n",
              "      <td>5.2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>50</td>\n",
              "      <td>1.020</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>?</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>?</td>\n",
              "      <td>...</td>\n",
              "      <td>38</td>\n",
              "      <td>6000</td>\n",
              "      <td>?</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>423</td>\n",
              "      <td>...</td>\n",
              "      <td>31</td>\n",
              "      <td>7500</td>\n",
              "      <td>?</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48</td>\n",
              "      <td>70</td>\n",
              "      <td>1.005</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>normal</td>\n",
              "      <td>abnormal</td>\n",
              "      <td>present</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>117</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>6700</td>\n",
              "      <td>3.9</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51</td>\n",
              "      <td>80</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>106</td>\n",
              "      <td>...</td>\n",
              "      <td>35</td>\n",
              "      <td>7300</td>\n",
              "      <td>4.6</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>60</td>\n",
              "      <td>90</td>\n",
              "      <td>1.015</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>74</td>\n",
              "      <td>...</td>\n",
              "      <td>39</td>\n",
              "      <td>7800</td>\n",
              "      <td>4.4</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>68</td>\n",
              "      <td>70</td>\n",
              "      <td>1.010</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>?</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>100</td>\n",
              "      <td>...</td>\n",
              "      <td>36</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>24</td>\n",
              "      <td>?</td>\n",
              "      <td>1.015</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>normal</td>\n",
              "      <td>abnormal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>410</td>\n",
              "      <td>...</td>\n",
              "      <td>44</td>\n",
              "      <td>6900</td>\n",
              "      <td>5</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>52</td>\n",
              "      <td>100</td>\n",
              "      <td>1.015</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>normal</td>\n",
              "      <td>abnormal</td>\n",
              "      <td>present</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>138</td>\n",
              "      <td>...</td>\n",
              "      <td>33</td>\n",
              "      <td>9600</td>\n",
              "      <td>4.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>53</td>\n",
              "      <td>90</td>\n",
              "      <td>1.020</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>abnormal</td>\n",
              "      <td>abnormal</td>\n",
              "      <td>present</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>70</td>\n",
              "      <td>...</td>\n",
              "      <td>29</td>\n",
              "      <td>12100</td>\n",
              "      <td>3.7</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows Ã— 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  age   bp     sg al su       rbc        pc         pcc          ba  bgr  \\\n",
              "0  48   80  1.020  1  0         ?    normal  notpresent  notpresent  121   \n",
              "1   7   50  1.020  4  0         ?    normal  notpresent  notpresent    ?   \n",
              "2  62   80  1.010  2  3    normal    normal  notpresent  notpresent  423   \n",
              "3  48   70  1.005  4  0    normal  abnormal     present  notpresent  117   \n",
              "4  51   80  1.010  2  0    normal    normal  notpresent  notpresent  106   \n",
              "5  60   90  1.015  3  0         ?         ?  notpresent  notpresent   74   \n",
              "6  68   70  1.010  0  0         ?    normal  notpresent  notpresent  100   \n",
              "7  24    ?  1.015  2  4    normal  abnormal  notpresent  notpresent  410   \n",
              "8  52  100  1.015  3  0    normal  abnormal     present  notpresent  138   \n",
              "9  53   90  1.020  2  0  abnormal  abnormal     present  notpresent   70   \n",
              "\n",
              "   ...  pcv   wbcc rbcc  htn   dm cad appet   pe  ane class  \n",
              "0  ...   44   7800  5.2  yes  yes  no  good   no   no   ckd  \n",
              "1  ...   38   6000    ?   no   no  no  good   no   no   ckd  \n",
              "2  ...   31   7500    ?   no  yes  no  poor   no  yes   ckd  \n",
              "3  ...   32   6700  3.9  yes   no  no  poor  yes  yes   ckd  \n",
              "4  ...   35   7300  4.6   no   no  no  good   no   no   ckd  \n",
              "5  ...   39   7800  4.4  yes  yes  no  good  yes   no   ckd  \n",
              "6  ...   36      ?    ?   no   no  no  good   no   no   ckd  \n",
              "7  ...   44   6900    5   no  yes  no  good  yes   no   ckd  \n",
              "8  ...   33   9600  4.0  yes  yes  no  good   no  yes   ckd  \n",
              "9  ...   29  12100  3.7  yes  yes  no  poor   no  yes   ckd  \n",
              "\n",
              "[10 rows x 25 columns]"
            ]
          },
          "execution_count": 208,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load data\n",
        "df = pd.read_csv('chronic_kidney_disease.csv', header=\"infer\")\n",
        "# names=['age', â€˜bpâ€™, â€˜sgâ€™, â€˜alâ€™, â€˜suâ€™, â€˜rbcâ€™, â€˜pcâ€™, â€˜pccâ€™, â€˜baâ€™, â€˜bgrâ€™, â€˜buâ€™, â€˜scâ€™, â€˜sodâ€™, â€˜potâ€™, \n",
        "# â€˜hemoâ€™, â€˜pcvâ€™, â€˜wcâ€™, â€˜rcâ€™, â€˜htnâ€™, â€˜dmâ€™, â€˜cadâ€™, â€˜appetâ€™, â€˜peâ€™, â€˜aneâ€™, â€˜classâ€™])\n",
        "# head of df\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEP1NVYMmJeI",
        "outputId": "ed9d6701-460b-49d3-ba9e-ad29b6198e9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "age      True\n",
              "bp       True\n",
              "sg       True\n",
              "al       True\n",
              "su       True\n",
              "rbc      True\n",
              "pc       True\n",
              "pcc      True\n",
              "ba       True\n",
              "bgr      True\n",
              "bu       True\n",
              "sc       True\n",
              "sod      True\n",
              "pot      True\n",
              "hemo     True\n",
              "pcv      True\n",
              "wbcc     True\n",
              "rbcc     True\n",
              "htn      True\n",
              "dm       True\n",
              "cad      True\n",
              "appet    True\n",
              "pe       True\n",
              "ane      True\n",
              "class    True\n",
              "dtype: bool"
            ]
          },
          "execution_count": 211,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Categorical boolean mask\n",
        "categorical_feature_mask = df.dtypes==object\n",
        "categorical_feature_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG4FwH0NmJeI",
        "outputId": "4bfb0d33-67c0-42d5-f085-a406008ea5fb"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'dtypes'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-209-4e95be920e1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Categorical boolean mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcategorical_feature_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# filter categorical columns using mask and turn it into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcategorical_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategorical_feature_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtypes'"
          ]
        }
      ],
      "source": [
        "# filter categorical columns using mask and turn it into a list\n",
        "categorical_cols = X.columns[categorical_feature_mask].tolist()\n",
        "categorical_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsrcb9WSmJeI",
        "outputId": "d72c9517-b17a-458b-9209-6e93c9c43697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train:\n",
            "        0  1\n",
            "0   small  1\n",
            "1   small  3\n",
            "2  medium  3\n",
            "3   large  2\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'small'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-203-7734cfc72489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'small'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m'medium'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'EXTRA-large'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"X_train:\\n{X_train}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"enc.fit_transform(X_train):\\n{enc.fit_transform(X_train)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"enc.transform(X_test):\\n{enc.transform(X_test)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    512\u001b[0m             return _transform_selected(\n\u001b[1;32m    513\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_fit_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m                 self._categorical_features, copy=True)\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/base.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[0;34m(X, transform, dtype, selected, copy, retain_order)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mXt\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msparse\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretain_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'small'"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "categorical_feature_mask = [True, False]\n",
        "# instantiate OneHotEncoder\n",
        "enc = OneHotEncoder(categorical_features = categorical_feature_mask,sparse = False, handle_unknown='ignore')\n",
        "# categorical_features = boolean mask for categorical columns\n",
        "# sparse = False output an array not sparse matrix\n",
        "X_train = pd.DataFrame([['small', 1], ['small', 3], ['medium', 3], ['large', 2]])\n",
        "X_test = [['small', 1.2],  ['medium', 4], ['EXTRA-large', 2]]\n",
        "print(f\"X_train:\\n{X_train}\")\n",
        "print(f\"enc.fit_transform(X_train):\\n{enc.fit_transform(X_train)}\")\n",
        "print(f\"enc.transform(X_test):\\n{enc.transform(X_test)}\")\n",
        "\n",
        "print(f\"enc.get_feature_names():\\n{enc.get_feature_names()}\") \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_yaluhjmJeI"
      },
      "outputs": [],
      "source": [
        "print(f\"enc.categories_{enc.categories_}\")\n",
        "\n",
        "print(f\"enc.categories_{enc.categories_}\")\n",
        "enc.transform([['Female', 1], ['Male', 4]]).toarray()\n",
        "\n",
        "\n",
        "enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
        "\n",
        "\n",
        "enc.get_feature_names()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Dy2PYTmJeI"
      },
      "source": [
        "### OHE case study: The breast cancer wisconsin dataset (classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvmbJJ-emJeJ",
        "outputId": "644aff17-a470-43f8-ce94-97ab48c7ef57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "execution_count": 184,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer(return_X_y=False)\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "print(y[[10, 50, 85]])\n",
        "#([0, 1, 0])\n",
        "list(data.target_names)\n",
        "#['malignant', 'benign']\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_yqZuUmmJeJ",
        "outputId": "f64581ec-4c23-47dc-da10-b5672a5ee2b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
              "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
              "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
              "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
              "       'smoothness error', 'compactness error', 'concavity error',\n",
              "       'concave points error', 'symmetry error',\n",
              "       'fractal dimension error', 'worst radius', 'worst texture',\n",
              "       'worst perimeter', 'worst area', 'worst smoothness',\n",
              "       'worst compactness', 'worst concavity', 'worst concave points',\n",
              "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.feature_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHApsG4ZmJeJ"
      },
      "source": [
        "Please [this  blog](https://medium.com/hugo-ferreiras-blog/dealing-with-categorical-features-in-machine-learning-1bb70f07262d) for more details of OHE when the validation/test have previously unseen unique values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHDqFhG0mJeJ"
      },
      "source": [
        "## HCDR preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpYV1vUUmJeJ"
      },
      "outputs": [],
      "source": [
        "# Split the provided training data into training and validationa and test\n",
        "# The kaggle evaluation test set has no labels\n",
        "#\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "use_application_data_ONLY = False #use joined data\n",
        "if use_application_data_ONLY:\n",
        "    # just selected a few features for a baseline experiment\n",
        "    selected_features = ['AMT_INCOME_TOTAL',  'AMT_CREDIT','DAYS_EMPLOYED','DAYS_BIRTH','EXT_SOURCE_1',\n",
        "        'EXT_SOURCE_2','EXT_SOURCE_3','CODE_GENDER', 'FLAG_OWN_REALTY','FLAG_OWN_CAR','NAME_CONTRACT_TYPE', \n",
        "                   'NAME_EDUCATION_TYPE','OCCUPATION_TYPE','NAME_INCOME_TYPE']\n",
        "    X_train = datasets[\"application_train\"][selected_features]\n",
        "    y_train = datasets[\"application_train\"]['TARGET']\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
        "    X_kaggle_test= datasets[\"application_test\"][selected_features]\n",
        "    # y_test = datasets[\"application_test\"]['TARGET']   #why no  TARGET?!! (hint: kaggle competition)\n",
        "\n",
        "selected_features = ['AMT_INCOME_TOTAL',  'AMT_CREDIT','DAYS_EMPLOYED','DAYS_BIRTH','EXT_SOURCE_1',\n",
        "        'EXT_SOURCE_2','EXT_SOURCE_3','CODE_GENDER', 'FLAG_OWN_REALTY','FLAG_OWN_CAR','NAME_CONTRACT_TYPE', \n",
        "                   'NAME_EDUCATION_TYPE','OCCUPATION_TYPE','NAME_INCOME_TYPE']\n",
        "y_train = X_train['TARGET']\n",
        "X_train = X_train[selected_features]\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
        "X_kaggle_test= X_kaggle_test[selected_features]\n",
        "# y_test = datasets[\"application_test\"]['TARGET']   #why no  TARGET?!! (hint: kaggle competition)\n",
        "\n",
        "    \n",
        "print(f\"X train           shape: {X_train.shape}\")\n",
        "print(f\"X validation      shape: {X_valid.shape}\")\n",
        "print(f\"X test            shape: {X_test.shape}\")\n",
        "print(f\"X X_kaggle_test   shape: {X_kaggle_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htRo3CcMmJeJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import re\n",
        "\n",
        "# Creates the following date features\n",
        "# But could do so much more with these features\n",
        "#    E.g., \n",
        "#      extract the domain address of the homepage and OneHotEncode it\n",
        "# \n",
        "# ['release_month','release_day','release_year', 'release_dayofweek','release_quarter']\n",
        "class prep_OCCUPATION_TYPE(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, features=\"OCCUPATION_TYPE\"): # no *args or **kargs\n",
        "        self.features = features\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # nothing else to do\n",
        "    def transform(self, X):\n",
        "        df = pd.DataFrame(X, columns=self.features)\n",
        "        #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit         \n",
        "        df['OCCUPATION_TYPE'] = df['OCCUPATION_TYPE'].apply(lambda x: 1. if x in ['Core Staff', 'Accountants', 'Managers', 'Sales Staff', 'Medicine Staff', 'High Skill Tech Staff', 'Realty Agents', 'IT Staff', 'HR Staff'] else 0.)   \n",
        "        #df.drop(self.features, axis=1, inplace=True)\n",
        "        return np.array(df.values)  #return a Numpy Array to observe the pipeline protocol\n",
        "    \n",
        "\n",
        "from sklearn.pipeline import make_pipeline \n",
        "features = [\"OCCUPATION_TYPE\"]\n",
        "def test_driver_prep_OCCUPATION_TYPE():\n",
        "    print(f\"X_train.shape: {X_train.shape}\\n\")\n",
        "    print(f\"X_train['name'][0:5]: \\n{X_train[features][0:5]}\")\n",
        "    test_pipeline = make_pipeline(prep_OCCUPATION_TYPE(features))\n",
        "    return(test_pipeline.fit_transform(X_train))\n",
        "          \n",
        "x = test_driver_prep_OCCUPATION_TYPE()\n",
        "print(f\"Test driver: \\n{test_driver_prep_OCCUPATION_TYPE()[0:10, :]}\")\n",
        "print(f\"X_train['name'][0:10]: \\n{X_train[features][0:10]}\")\n",
        "\n",
        "\n",
        "# QUESTION, should we lower case df['OCCUPATION_TYPE'] as Sales staff != 'Sales Staff'? (hint: YES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebon6-nMmJeJ"
      },
      "outputs": [],
      "source": [
        "# Create a class to select numerical or categorical columns \n",
        "# since Scikit-Learn doesn't handle DataFrames yet\n",
        "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, attribute_names):\n",
        "        self.attribute_names = attribute_names\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X[self.attribute_names].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjt3TdsZmJeJ"
      },
      "outputs": [],
      "source": [
        "# Identify the numeric features we wish to consider. \n",
        "num_attribs = [\n",
        "    'AMT_INCOME_TOTAL',  'AMT_CREDIT','DAYS_EMPLOYED','DAYS_BIRTH','EXT_SOURCE_1',\n",
        "    'EXT_SOURCE_2','EXT_SOURCE_3']\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('selector', DataFrameSelector(num_attribs)),\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "# Identify the categorical features we wish to consider.\n",
        "cat_attribs = ['CODE_GENDER', 'FLAG_OWN_REALTY','FLAG_OWN_CAR','NAME_CONTRACT_TYPE', \n",
        "               'NAME_EDUCATION_TYPE','OCCUPATION_TYPE','NAME_INCOME_TYPE']\n",
        "\n",
        "# Notice handle_unknown=\"ignore\" in OHE which ignore values from the validation/test that\n",
        "# do NOT occur in the training set\n",
        "cat_pipeline = Pipeline([\n",
        "        ('selector', DataFrameSelector(cat_attribs)),\n",
        "        #('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "data_prep_pipeline = FeatureUnion(transformer_list=[\n",
        "        (\"num_pipeline\", num_pipeline),\n",
        "        (\"cat_pipeline\", cat_pipeline),\n",
        "    ])\n",
        "              \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMpF_25MmJeJ"
      },
      "outputs": [],
      "source": [
        "list(datasets[\"application_train\"].columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6UIvyq0mJeJ"
      },
      "source": [
        "# Baseline Model\n",
        "\n",
        "To get a baseline, we will use some of the features after being preprocessed through the pipeline.\n",
        "The baseline model is a logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P3Ajgj9mJeJ"
      },
      "outputs": [],
      "source": [
        "def pct(x):\n",
        "    return round(100*x,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HklpLkf-mJeJ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    expLog\n",
        "except NameError:\n",
        "    expLog = pd.DataFrame(columns=[\"exp_name\", \n",
        "                                   \"Train Acc\", \n",
        "                                   \"Valid Acc\",\n",
        "                                   \"Test  Acc\",\n",
        "                                   \"Train AUC\", \n",
        "                                   \"Valid AUC\",\n",
        "                                   \"Test  AUC\"\n",
        "                                  ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-YUUkxOmJeJ"
      },
      "outputs": [],
      "source": [
        "%%time \n",
        "np.random.seed(42)\n",
        "full_pipeline_with_predictor = Pipeline([\n",
        "        (\"preparation\", data_prep_pipeline),\n",
        "        (\"linear\", LogisticRegression())\n",
        "    ])\n",
        "model = full_pipeline_with_predictor.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4RKU_kgmJeK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "np.round(accuracy_score(y_train, model.predict(X_train)), 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PTA_0TFmJeK"
      },
      "source": [
        "## Evaluation metrics\n",
        "Submissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.\n",
        "\n",
        "The SkLearn `roc_auc_score` function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in one number. \n",
        "\n",
        "```python\n",
        "from sklearn.metrics import roc_auc_score\n",
        ">>> y_true = np.array([0, 0, 1, 1])\n",
        ">>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
        ">>> roc_auc_score(y_true, y_scores)\n",
        "0.75\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3AWfcazmJeK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK5kBzrDmJeK"
      },
      "outputs": [],
      "source": [
        "exp_name = f\"Baseline_{len(selected_features)}_features\"\n",
        "expLog.loc[len(expLog)] = [f\"{exp_name}\"] + list(np.round(\n",
        "               [accuracy_score(y_train, model.predict(X_train)), \n",
        "                accuracy_score(y_valid, model.predict(X_valid)),\n",
        "                accuracy_score(y_test, model.predict(X_test)),\n",
        "                roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]),\n",
        "                roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1]),\n",
        "                roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])],\n",
        "    4)) \n",
        "expLog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUZRqifnmJeK"
      },
      "source": [
        "## Submission File Prep\n",
        "\n",
        "For each SK_ID_CURR in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:\n",
        "\n",
        "```python \n",
        "SK_ID_CURR,TARGET\n",
        "100001,0.1\n",
        "100005,0.9\n",
        "100013,0.2\n",
        "etc.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZwiQzMWmJeK"
      },
      "outputs": [],
      "source": [
        "test_class_scores = model.predict_proba(X_kaggle_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3tGKBbNmJeK"
      },
      "outputs": [],
      "source": [
        "test_class_scores[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkmAjVvBmJeK"
      },
      "outputs": [],
      "source": [
        "# Submission dataframe\n",
        "submit_df = datasets[\"application_test\"][['SK_ID_CURR']]\n",
        "submit_df['TARGET'] = test_class_scores\n",
        "\n",
        "submit_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfhYvThRmJeK"
      },
      "outputs": [],
      "source": [
        "submit_df.to_csv(\"submission.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3kc-OZCmJeK"
      },
      "source": [
        "# Kaggle submission via the command line API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMUXN7OjmJeK"
      },
      "outputs": [],
      "source": [
        "! kaggle competitions submit -c home-credit-default-risk -f submission.csv -m \"baseline submission\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEKmkgrZmJeK"
      },
      "source": [
        "## report submission\n",
        "\n",
        "Click on this [link](https://www.kaggle.com/c/home-credit-default-risk/submissions?sortBy=date&group=all&page=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gn97AtrmJeK"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEShnlpfmJeK"
      },
      "source": [
        "# Write-up \n",
        "For this phase of the project, you will need to submit a write-up summarizing the work you did. \n",
        "The write-up form is available on Canvas (Modules-> Module 12.1 - Course Project - Home Credit Default Risk (HCDR)-> FP Phase 2 (HCDR) : write-up form ). It has the following sections: \n",
        "## Abstract\n",
        "Please provide an abstract summarizing the work you did (150 words)\n",
        "## Introduction\n",
        "## Feature Engineering and transformers\n",
        "Please explain the work you conducted on feature engineering and transformers. \n",
        "Please include code sections when necessary as well as images or any relevant material\n",
        "## Pipelines\n",
        "Please explain the pipelines you created for this project and how you used them\n",
        "Please include code sections when necessary as well as images or any relevant material\n",
        "## Experimental results\n",
        "Please present the results of the various experiments that you conducted. The results should be shown in a table or image. Try to include the different details for each experiment.  \n",
        "\n",
        "Please include code sections when necessary as well as images or any relevant material\n",
        "## Discussion\n",
        "Discuss  & analyze your different experimental results  \n",
        "\n",
        "Please include code sections when necessary as well as images or any relevant material\n",
        "## Conclusion\n",
        "## Kaggle Submission\n",
        "Please provide a screenshot of your best kaggle submission.   \n",
        "The screenshot should show the different details of the submission and not just the score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4OaVivmmJeK"
      },
      "source": [
        "# References\n",
        "\n",
        "Some of the material in this notebook has been adopted from [here](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction/notebook)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rdnVFZ0mJeK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzsGDGormJeK"
      },
      "source": [
        "#  TODO: Predicting Loan Repayment with Automated Feature Engineering in Featuretools\n",
        "\n",
        "Read the following:\n",
        "* feature engineering via Featuretools library: \n",
        "  * https://github.com/Featuretools/predict-loan-repayment/blob/master/Automated%20Loan%20Repayment.ipynb\n",
        "* https://www.analyticsvidhya.com/blog/2018/08/guide-automated-feature-engineering-featuretools-python/\n",
        "* feature engineering paper: https://dai.lids.mit.edu/wp-content/uploads/2017/10/DSAA_DSM_2015.pdf\n",
        "* https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "329.631px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "HCDR_baseLine_submission_with_numerical_and_cat_features_to_kaggle.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}